Sanjana Ilango, ilango@usc.edu
        My project leverages sentiment analysis to classify news article headlines as either sarcastic or not sarcastic. To do so, it uses a pretrained BERT model overlaid by a single binary classification layer. For this task, I chose the Kaggle dataset titled “News Headlines Dataset For Sarcasm Detection”. This dataset includes three columns: the article link, the headline, and whether the headline is sarcastic or not. When cleaning the data, I removed the article links entirely to prevent any bias towards certain news sources and keep the focus on the text of the headline. Additionally, I dropped all null entries to prevent syntactical errors. Next, I preprocessed the data by using the BERT tokenizer to convert each headline into a series of model-recognizable tokens. This reduced the length and complexity of the headlines, making it easier for the model to process the text and make predictions.
        For model development, I used the pretrained BERT model known as ‘BertForSequence Classification’. This model is suited for my task of detecting sarcasm because it is overlaid by a single layer which can be adapted for binary classification. The training procedure used 80% of the data (train-validation-test split was 80-10-10). With batch sizes of 32, randomly sampled batches were taken and a forward pass was made. Then, the resulting loss was calculated and used to perform backpropagation and update the gradients of the optimizer. This process was repeated for 4 epochs, thus concluding the training process. The Adam optimizer was used due to its increased efficiency compared to SGD alone. Additionally, a very low learning rate was used at 2e-5. This was to account for the complexity of the dataset which included long strings of text. The batch sizes and number of epochs were chosen to capture the complexity of the data without overfitting to the training dataset. This was confirmed by tracking the validation error rate.
        In terms of results, the model appeared to perform well on the test set which was 10% of the total data. The metrics I chose to track were the Matthew's correlation coefficient (MCC), precision, recall, f1-score, and accuracy. After the training, validation, and testing procedures, the model returned an MCC score of 0.863. Additionally, it had a precision of 93%, recall of 95%, f1-score of 94%, and finally an accuracy of 93%.
        Overall, my dataset provided a good starting point for the task of detecting and classifying sarcasm in news headlines. It included headlines from two different news sources, but a larger dataset with more news sources would have provided more diverse information to the model. The training procedures and model architecture seem to be strongly suited to the task, particularly with using a pretrained tokenizer and model. The metrics I chose were comprehensive and measured several aspects of the results. My efforts could be extended in the future to do other types of text-based analysis. For example, doing fake news detection could help prevent its spread online. Additionally, doing sentiment analysis on customer reviews could help companies improve their products. However, there are limitations to my current methods. For example, I only have access to a certain amount of computational power. Additionally, I am unsure as to how much longer pieces of text can be effectively synthesized by the model.
If I were to continue this project, my immediate next step would be to diversify the dataset. Then, I would modify and update the hyperparameters to perform effectively on this more generalized dataset. I may also shift the project by creating a similarly-structured model and dataset, but focusing on social media. For example, I could take a text dataset of Reddit comments and do sarcasm sentiment analysis. These new ways of approaching the task would help expand the project and give it more real world value.